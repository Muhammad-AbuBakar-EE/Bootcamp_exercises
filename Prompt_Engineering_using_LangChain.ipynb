{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e95f0072",
      "metadata": {
        "id": "e95f0072"
      },
      "source": [
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef463a2",
      "metadata": {
        "id": "0ef463a2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b143a576",
      "metadata": {
        "id": "b143a576"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face\n",
        "\n",
        "We first need to install additional prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "badb1fa1",
      "metadata": {
        "id": "badb1fa1",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823bd325",
      "metadata": {
        "id": "823bd325"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea18d93",
      "metadata": {
        "id": "7ea18d93"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f256aa",
      "metadata": {
        "id": "83f256aa"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WNK5JoRoUkhX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNK5JoRoUkhX",
        "outputId": "b37008a1-9dc8-4857-c035-4e84886589d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ece2701",
      "metadata": {
        "id": "5ece2701"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm.auto\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.utils._deprecation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd3d6ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd3d6ad",
        "outputId": "91a61891-1609-4613-9bad-57bceb96957c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "six\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "#     repo_id=\"google/flan-t5-xl\",\n",
        "#     repo_id=\"google/flan-t5-base\",\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "# question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "question = \"How many balls are there in a cricket over?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96351d3b",
      "metadata": {
        "id": "96351d3b"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9166203a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9166203a",
        "outputId": "492f326c-bcc1-442f-d4dc-01557f702384"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='san francisco 49ers')], [Generation(text='84')], [Generation(text='samuel harris')], [Generation(text='four')]], llm_output=None, run=[RunInfo(run_id=UUID('bf1d543b-4c93-46e5-908e-71652e1ba7a7')), RunInfo(run_id=UUID('d165cbf3-aa8c-4289-aceb-00d94cf6cf9c')), RunInfo(run_id=UUID('911dc372-8527-4090-bf3a-e160aacdf58d')), RunInfo(run_id=UUID('cff77714-9ed8-4b43-9e6a-451087e41acf'))])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e288c3",
      "metadata": {
        "id": "16e288c3"
      },
      "source": [
        "It is a LLM, so we can try feeding in all questions at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0600bdea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0600bdea",
        "outputId": "f8d8603d-22a6-4d6d-978b-02be5c8eb671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one\n"
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b123c1da",
      "metadata": {
        "id": "b123c1da"
      },
      "source": [
        "But with this model it doesn't work too well, we'll see this approach works better with different models soon."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15daae92",
      "metadata": {
        "id": "15daae92"
      },
      "source": [
        "## OpenAI\n",
        "\n",
        "Start by installing additional prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c4379e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c4379e",
        "outputId": "9f90cdaa-43fa-4f81-cc41-990b7fdd95b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7918e43",
      "metadata": {
        "id": "c7918e43"
      },
      "source": [
        "We can also use OpenAI's generative models. The process is similar, we need to\n",
        "give our API key which can be retrieved by signing up for an account on the\n",
        "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f250b8",
      "metadata": {
        "id": "30f250b8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c711ef",
      "metadata": {
        "id": "63c711ef"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97bdb8e",
      "metadata": {
        "id": "a97bdb8e"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355c33a9",
      "metadata": {
        "id": "355c33a9"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name='text-davinci-003')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163af6b2",
      "metadata": {
        "id": "163af6b2"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain.llms import AzureOpenAI\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\",\n",
        "    model_name=\"text-davinci-003\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4db047b",
      "metadata": {
        "id": "f4db047b"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wNUdWx_UcKWS",
      "metadata": {
        "id": "wNUdWx_UcKWS"
      },
      "outputs": [],
      "source": [
        "question  = \"What is the colour of sky on Mars?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458121ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "458121ce",
        "outputId": "1d315299-ed97-4918-c393-fce62945d58a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The sky on Mars is a pinkish-orange color due to the presence of iron oxide in the atmosphere.\n"
          ]
        }
      ],
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86672b04",
      "metadata": {
        "id": "86672b04"
      },
      "source": [
        "The same works again for multiple questions using `generate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd409712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd409712",
        "outputId": "e051a274-e437-44c0-c87c-b09c26f6fbc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 6 ft 4 inches is approximately 193.04 centimeters.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Eugene Cernan was the twelfth and final person to walk on the moon. He was part of the Apollo 17 mission in 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 75, 'completion_tokens': 61, 'total_tokens': 136}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('5b79f838-6dc4-4250-9580-abfba84ca1ac')), RunInfo(run_id=UUID('065b0d65-fc47-46ab-9b7d-418b6249331a')), RunInfo(run_id=UUID('ea424824-05b7-40fb-b886-ac38247cb72c')), RunInfo(run_id=UUID('1698a6ed-f43d-4626-b7d6-d2c9016fa0c8'))])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b531db0",
      "metadata": {
        "id": "8b531db0"
      },
      "source": [
        "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fcc8cd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcc8cd7",
        "outputId": "512ba587-d84b-4580-d77b-afd84a47c5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. The New Orleans Saints \n",
            "2. 193.04 centimeters \n",
            "3. Harrison Schmitt \n",
            "4. None \n",
            "5. Zhores I. Alferov, Herbert Kroemer, and Jack St. Clair Kilby\n"
          ]
        }
      ],
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    \"How many eyes does a blade of grass have?\",\n",
        "    \"Names all of the winners of the Nobel Prize in the field of physics in the year 2000?\"\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf4a813",
      "metadata": {
        "id": "9bf4a813"
      },
      "source": [
        "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed43323b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed43323b",
        "outputId": "6feac406-e1f9-4d6f-a976-36455384c788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
            "6 ft 4 inches is 193.04 centimeters.\n",
            "The 12th person on the moon was Charles Duke.\n",
            "A blade of grass does not have any eyes.\n",
            "1km equals to 0.621371 miles.\n"
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\" +\n",
        "    \"1km equals to how many miles?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QD5vqJj9yh6-",
      "metadata": {
        "id": "QD5vqJj9yh6-"
      },
      "outputs": [],
      "source": [
        "# print(openai(qs_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8464ff5f",
      "metadata": {
        "id": "8464ff5f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987670cd",
      "metadata": {
        "id": "987670cd"
      },
      "source": [
        "\n",
        "# Prompt Engineering using LangChain\n",
        "\n",
        "We'll explore the fundamentals of prompt engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdca342c",
      "metadata": {
        "id": "bdca342c"
      },
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b62298e-2181-4e73-bb40-77e20c655231",
      "metadata": {
        "id": "3b62298e-2181-4e73-bb40-77e20c655231"
      },
      "source": [
        "## Prompting Principles\n",
        "- **Principle 1: Write clear and specific instructions**\n",
        "- **Principle 2: Give the model time to “think”**\n",
        "\n",
        "### Tactics\n",
        "\n",
        "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
        "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc9ca50",
      "metadata": {
        "id": "dfc9ca50"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9587ff64",
      "metadata": {
        "id": "9587ff64"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a GPT-3 model. We will use the LangChain library but you can also use the `openai` library directly. In both cases, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
        "\n",
        "We initialize a `text-davinci-003` model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6faff5e",
      "metadata": {
        "id": "f6faff5e"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# initialize the models\n",
        "openai = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    openai_api_key=\"OPENAI_API_KEY\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3d3877",
      "metadata": {
        "id": "9c3d3877"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03264b76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03264b76",
        "outputId": "f6637fb9-2b61-4f31-b000-6c665d0f5b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0d164f",
      "metadata": {
        "id": "8a0d164f"
      },
      "source": [
        "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a `PromptTemplate` with a single input variable `query`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431ef795",
      "metadata": {
        "id": "431ef795"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943922ab",
      "metadata": {
        "id": "943922ab"
      },
      "source": [
        "Now we can insert the user's `query` to the prompt template via the `query` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142b01c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "142b01c8",
        "outputId": "5b2d11f5-979a-4a7e-96ec-57602368c96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d523b79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d523b79",
        "outputId": "18c87720-bd45-4fd7-fc53-657fc5825ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The `transformers` library from Hugging Face, the `openai` library from OpenAI, and the `cohere` library from Cohere.\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d711fa",
      "metadata": {
        "id": "28d711fa"
      },
      "source": [
        "This is just a simple implementation, that we can easily replace with f-strings (like `f\"insert some custom text '{custom_text}' etc\"`). But using LangChain's `PromptTemplate` object we're able to formalize the process, add multiple parameters, and build the prompts in an object-oriented way.\n",
        "\n",
        "Yet, these are not the only benefits of using LangChains prompt tooling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc5e72b",
      "metadata": {
        "id": "7bc5e72b"
      },
      "source": [
        "## Few Shot Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8806ac8",
      "metadata": {
        "id": "f8806ac8"
      },
      "source": [
        "Another useful feature offered by LangChain is the `FewShotPromptTemplate` object. This is ideal for what we'd call *few-shot learning* using our prompts.\n",
        "\n",
        "To give some context, the primary sources of \"knowledge\" for LLMs are:\n",
        "\n",
        "* **Parametric knowledge** — the knowledge has been learned during model training and is stored within the model weights.\n",
        "\n",
        "* **Source knowledge** — the knowledge is provided within model input at inference time, i.e. via the prompt.\n",
        "\n",
        "The idea behind `FewShotPromptTemplate` is to provide few-shot training as **source knowledge**. To do this we add a few examples to our prompts that the model can read and then apply to our user's input."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd72af9",
      "metadata": {
        "id": "1cd72af9"
      },
      "source": [
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7585b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7585b5",
        "outputId": "72faf85c-9ace-4661-ed6b-7026f5f1739f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The meaning of life is to be curious, creative, and to live life to the fullest!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "openai.temperature = 1.0  # increase creativity/randomness of output\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc63e29",
      "metadata": {
        "id": "0bc63e29"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c6f21e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15c6f21e",
        "outputId": "4ef941e2-572c-442e-fb93-2a503b77f166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " To laugh, love, and live life to the fullest!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: How you can enjoy the little moments?\n",
        "AI: Those are your moments what can I suggest.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d942de41",
      "metadata": {
        "id": "d942de41"
      },
      "source": [
        "We now get a much better response and we did this via *few-shot learning* by adding a few examples via our source knowledge.\n",
        "\n",
        "Now, to implement this with LangChain's `FewShotPromptTemplate` we need to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e7b73",
      "metadata": {
        "id": "f53e7b73"
      },
      "outputs": [],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07128aad",
      "metadata": {
        "id": "07128aad"
      },
      "source": [
        "Now let's see what this creates when we feed in a user query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3e57b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f3e57b3",
        "outputId": "806d6800-13d3-4029-d04f-16af675b3b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples:\n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b325c15f",
      "metadata": {
        "id": "b325c15f"
      },
      "source": [
        "And to generate with this we just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcc1508",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bcc1508",
        "outputId": "9818f877-dcb7-4d70-d017-f6124256e79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Life is not meant to be figured out, it's meant to be enjoyed!\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    few_shot_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5f2f52",
      "metadata": {
        "id": "6c5f2f52"
      },
      "source": [
        "Again, another good response.\n",
        "\n",
        "However, this does some somewhat convoluted. Why go through all of the above with `FewShotPromptTemplate`, the `examples` dictionary, etc — when we can do the same with a single f-string.\n",
        "\n",
        "Well this approach is more robust and contains some nice features. One of those is the ability to include or exclude examples based on the length of our query.\n",
        "\n",
        "This is actually very important because the max length of our prompt and generation output is limited. This limitation is the *max context window*, and is simply the length of our prompt + length of our generation (which we define via `max_tokens`).\n",
        "\n",
        "So we must try to maximize the number of examples we give to the model as few-shot learning examples, while ensuring we don't exceed the maximum context window or increase processing times excessively.\n",
        "\n",
        "Let's see how the dynamic inclusion/exclusion of examples works. First we need more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263d426b",
      "metadata": {
        "id": "263d426b"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
        "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite color?\",\n",
        "        \"answer\": \"79\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite food?\",\n",
        "        \"answer\": \"Carbon based lifeforms\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"What is the best thing in the world?\",\n",
        "        \"answer\": \"The perfect pizza.\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"If you could do anything in the world what would you do?\",\n",
        "        \"answer\": \"Take over the world, of course!\"\n",
        "    }, {\n",
        "        \"query\": \"Where should I travel?\",\n",
        "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f55de0b",
      "metadata": {
        "id": "0f55de0b"
      },
      "source": [
        "Then rather than using the `examples` list of dictionaries directly we use a `LengthBasedExampleSelector` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9e1d76",
      "metadata": {
        "id": "3c9e1d76"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # this sets the max length that examples should be\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390dcd4b",
      "metadata": {
        "id": "390dcd4b"
      },
      "source": [
        "Note that the `max_length` is measured as a split of words between newlines and spaces, determined by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c33c1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1c33c1b",
        "outputId": "73b6c27a-9d31-4486-c308-ec9103306e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
        "\n",
        "words = re.split('[\\n ]', some_text)\n",
        "print(words, len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7f737d",
      "metadata": {
        "id": "4f7f737d"
      },
      "source": [
        "Then we use the selector to initialize a `dynamic_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470068df",
      "metadata": {
        "id": "470068df"
      },
      "outputs": [],
      "source": [
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e37698",
      "metadata": {
        "id": "49e37698"
      },
      "source": [
        "We can see that the number of included prompts will vary based on the length of our query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027c0021",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "027c0021",
        "outputId": "3dc461c1-f8d4-4811-b816-f43bfe11c5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples:\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: How do birds fly?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9d23d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3f9d23d",
        "outputId": "e30e923f-ab2d-4b4b-cce4-0197a5c75b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " With a little help from their friends—the wind.\n"
          ]
        }
      ],
      "source": [
        "query = \"How do birds fly?\"\n",
        "\n",
        "print(openai(\n",
        "    dynamic_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d127973",
      "metadata": {
        "id": "5d127973"
      },
      "source": [
        "Or if we ask a longer question..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae5744e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae5744e2",
        "outputId": "718dd6bb-8ae6-4ffd-a324-25a1219029e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples:\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
        "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
        "what is the best way to do that?\"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d4cbaf",
      "metadata": {
        "id": "04d4cbaf"
      },
      "source": [
        "With this we've limited the number of examples being given within the prompt. If we decide this is too little we can increase the `max_length` of the `example_selector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9918f2ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9918f2ac",
        "outputId": "e60b37a0-150d-45d3-d54c-a3ec2cbc92f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples:\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: What is the weather like today?\n",
            "AI: Cloudy with a chance of memes.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100  # increased max length\n",
        ")\n",
        "\n",
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7dc2918",
      "metadata": {
        "id": "b7dc2918"
      },
      "source": [
        "These are just a few of the prompt tooling available in LangChain. For example, there is actually an entire other set of example selectors beyond the `LengthBasedExampleSelector`. We'll cover them in detail in upcoming Labs, or you can read about them in the [LangChain docs](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/example_selectors.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae0ee8f",
      "metadata": {
        "id": "4ae0ee8f"
      },
      "source": [
        "# Prompt Engineering using OpenAI Library\n",
        "\n",
        "Now we'll explore the fundamentals of prompt engineering using `openai` library rather than langchain which we'll be using throughout these examples. However, note that we can use other LLMs here, like those offered by Cohere or open source alternatives available via Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db18784e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db18784e",
        "outputId": "56b50297-2b6c-4888-e0a0-4a15650b9128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai==0.27.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20d51f4",
      "metadata": {
        "id": "d20d51f4"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eea1dff",
      "metadata": {
        "id": "9eea1dff"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a GPT-3 model. For this, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
        "\n",
        "We initialize a `text-davinci-003` model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cc75b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cc75b3",
        "outputId": "8766d2ae-6e1c-4d1b-b38e-2a0847147cf8",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7bac1817fec0> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-babbage-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-babbage-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-babbage-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-davinci-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-davinci-edit-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-instruct-beta\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-ada-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-instruct-beta\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-babbage-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-davinci-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-curie-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-davinci-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-code-search-code\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-code-search-code\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-ada-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-ada-text-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-code-search-text\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-babbage-code-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-code-search-text\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-curie-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-edit-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-babbage-text-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-ada-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-ada-code-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-ada-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-16k\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-curie-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-003\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-curie-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4-0314\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    }\n",
              "  ]\n",
              "}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb169df",
      "metadata": {
        "id": "adb169df"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be96e1b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be96e1b0",
        "outputId": "c484ff15-b3dc-4f8c-e73a-d02f23e95a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "# now query text-davinci-003\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f660be48",
      "metadata": {
        "id": "f660be48"
      },
      "source": [
        "Alternatively, if we do have the correct information withing the `context`, the model should reply with `\"I don't know\"`, let's try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a95085",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a95085",
        "outputId": "d0c4432c-c3da-46ed-ac30-340ae9ef5522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Libraries are places full of books.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a78dd3",
      "metadata": {
        "id": "50a78dd3"
      },
      "source": [
        "Perfect, our instructions are being understood by the model. In most real use-cases we won't be providing the external information / context to the model manually. Instead, it will be an automatic process using something like [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) to retrieve relevant information from an external source.\n",
        "\n",
        "For now, that's beyond the scope of what we're exploring here, you can find more on that in the link above.\n",
        "\n",
        "In summary, a prompt often consists of those four components: instructions, context(s), user input, and the output indicator. Now we'll take a look at creative vs. stricter generation.\n",
        "\n",
        "## Generation Temperature\n",
        "\n",
        "The `temperature` parameter used in generation models tells us how \"random\" the model can be. It represents the probability of a model to choose a word which is *not* the first choice of the model.\n",
        "\n",
        "This works because the model is actually assigning a probability prediction across all tokens within it's vocabulary with each _\"step\"_ of the model (each new word or sub-word).\n",
        "\n",
        "With each new step forwards the model considers the previous tokens fed into the model, creates an embedding by encoding the information from these tokens over many model encoder layers, then passes this encoding to a decoder. The decoder then predicts the probability of each token that the model knows (ie is within the model *vocabulary*) based on the information encoded within the embedding.\n",
        "\n",
        "At a temperature of `0.0` the decoder will always select the top predicted token. At a temperature of `1.0` the model will always select a word that *is predicted* considering it's assigned probability.\n",
        "\n",
        "Considering all of this, if we have a conservative, fact based Q&A like in the previous example, it makes sense to set a lower `temperature`. However, if we're wanting to produce some creative writing or chatbot conversations, we might want to experiment and increase `temperature`. Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686bb185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686bb185",
        "outputId": "5c089b49-cca7-4d59-9680-90e37f543da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oh, just hanging out and having a good time. What about you?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0  # set the temperature, default is 1\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76765615",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76765615",
        "outputId": "96cb8d8c-12b7-4235-ac1d-ce3a004f3c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Just hanging out and cracking jokes. What about you?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=512,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ded51cc",
      "metadata": {
        "id": "4ded51cc"
      },
      "source": [
        "The second response is far more creative and demonstrates the type of difference we can expect between low `temperature` and high `temperature` generations.\n",
        "\n",
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88af1e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88af1e7b",
        "outputId": "9afc7d47-10a0-4a83-f541-6e4d1ed1d72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The meaning of life is whatever you make it.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bd8bdd",
      "metadata": {
        "id": "f0bd8bdd"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2899ba4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2899ba4",
        "outputId": "c73693b9-7a96-4553-f3a1-4946ebf6f286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The meaning of life is to keep on living life to the fullest.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48aa103f",
      "metadata": {
        "id": "48aa103f"
      },
      "source": [
        "This is a much better response and the way we did this was by providing a *few* examples that included the example inputs and outputs that we'd expect. We refer to this as _\"few-shot learning\"_.\n",
        "\n",
        "## Adding Multiple Contexts\n",
        "\n",
        "In some use-cases like question-answering we can use an external source of information to improve the reliability or *factfulness* of model responses. We refer to this information as _\"source knowledge\"_, which is any knowledge fed into the model via the input prompt.\n",
        "\n",
        "We'll create a list of \"dummy\" external information. In reality we'd likely use [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) or some form of information grabbing APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f99fb5",
      "metadata": {
        "id": "d4f99fb5"
      },
      "outputs": [],
      "source": [
        "contexts = [\n",
        "    (\n",
        "        \"Large Language Models (LLMs) are the latest models used in NLP. \" +\n",
        "        \"Their superior performance over smaller models has made them incredibly \" +\n",
        "        \"useful for developers building NLP enabled applications. These models \" +\n",
        "        \"can be accessed via Hugging Face's `transformers` library, via OpenAI \" +\n",
        "        \"using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    ),\n",
        "    (\n",
        "        \"To use OpenAI's GPT-3 model for completion (generation) tasks, you \" +\n",
        "        \"first need to get an API key from \" +\n",
        "        \"'https://beta.openai.com/account/api-keys'.\"\n",
        "    ),\n",
        "    (\n",
        "        \"OpenAI's API is accessible via Python using the `openai` library. \" +\n",
        "        \"After installing the library with pip you can use it as follows: \\n\" +\n",
        "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
        "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
        "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
        "    ),\n",
        "    (\n",
        "        \"The OpenAI endpoint is available for completion tasks via the \" +\n",
        "        \"LangChain library. To use it, first install the library with \" +\n",
        "        \"`pip install langchain openai`. Then, import the library and \" +\n",
        "        \"initialize the model as follows: \\n\" +\n",
        "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
        "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
        "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e042ddfd",
      "metadata": {
        "id": "e042ddfd"
      },
      "source": [
        "We would feed this external information into our prompt between the initial *instructions* and the *user input*. For OpenAI models it's recommended to separate the contexts from the rest of the prompt using `###` or `\"\"\"`, and each independent context can be separated with a few newlines and `##`, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc802140",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc802140",
        "outputId": "244da4d0-f94c-43f2-d5c0-33694dd0888a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the contexts below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "###\n",
            "\n",
            "Contexts:\n",
            "Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "##\n",
            "\n",
            "To use OpenAI's GPT-3 model for completion (generation) tasks, you first need to get an API key from 'https://beta.openai.com/account/api-keys'.\n",
            "\n",
            "##\n",
            "\n",
            "OpenAI's API is accessible via Python using the `openai` library. After installing the library with pip you can use it as follows: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = \n",
            "'<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "##\n",
            "\n",
            "The OpenAI endpoint is available for completion tasks via the LangChain library. To use it, first install the library with `pip install langchain openai`. Then, import the library and initialize the model as follows: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n",
            "\n",
            "###\n",
            "\n",
            "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
            "using Python from start to finish\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
        "\n",
        "print(f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833a60b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "833a60b5",
        "outputId": "f5e46243-d37c-469b-e546-d24f164731a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "2. from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5286521",
      "metadata": {
        "id": "e5286521"
      },
      "source": [
        "Not bad, but are these contexts actually helping? Maybe the model is able to answer these questions without the additional information (source knowledge) as is able to rely solely on information stored within the model's internal parameters (parametric knowledge). Let's ask again without the external information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2e8c57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c2e8c57",
        "outputId": "c31d317c-17c0-43bf-e810-71296ab95daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Using OpenAI's GPT-3 model with Python to generate text: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate text using the GPT-3 model\n",
            "\n",
            "2. Using OpenAI's GPT-3 model with Python to generate images: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate images using the GPT-3 model\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96af80fe",
      "metadata": {
        "id": "96af80fe"
      },
      "source": [
        "These are not really what we asked for, and are definitely not very specific. So clearly adding some source knowledge to our prompts can result in some much better results.\n",
        "\n",
        "## Maximum Prompt Sizes\n",
        "\n",
        "Considering that we might want to feed in external information to our prompts, they can naturally become quite large. With this we need to ask how large our prompts can be, because there is a maxiumum size.\n",
        "\n",
        "The maxiumum *context window* of a LLM refers to tokens across both the *prompt* and the *completion* text. For `text-davinci-003` this is `4097` tokens.\n",
        "\n",
        "We can set the maximum completion length of our model using `openai.max_tokens = 123`. However, measuring the total number of input tokens is more complex.\n",
        "\n",
        "Because tokens don't map directly to words, we can only measure the number of tokens from text by actually tokenizing the text. GPT models use [OpenAI's TikToken tokenizer](https://github.com/openai/tiktoken). We can install the library via Pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05964fe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05964fe0",
        "outputId": "5c23b669-2442-4ddd-a032-563a872e9aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385813e6",
      "metadata": {
        "id": "385813e6"
      },
      "source": [
        "Taking the earlier prompt we can measure the number of tokens like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee3f86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ee3f86a",
        "outputId": "aa27a6a6-4482-405e-b0a9-fc4ff6c721ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{'##'.join(contexts)}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "encoder_name = 'p50k_base'\n",
        "tokenizer = tiktoken.get_encoding(encoder_name)\n",
        "\n",
        "len(tokenizer.encode(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7879826",
      "metadata": {
        "id": "f7879826"
      },
      "source": [
        "When feeding this prompt into `text-davinci-003` it will use `412` of our maximum context window of `4097`, leaving us with `4097 - 412 == 3685` tokens for our completion.\n",
        "\n",
        "---\n",
        "\n",
        "*Not all OpenAI models use the `p50k_base` encoder, a table of different encoders for different models can be found [here](), as of this writing they are:*\n",
        "\n",
        "| Encoding name | OpenAI models |\n",
        "| --- | --- |\n",
        "| `gpt2` (or `r50k_base`) | Most GPT-3 models (and GPT-2) |\n",
        "| `p50k_base` | Code models, `text-davinci-002`, `text-davinci-003` |\n",
        "| `cl100k_base` | `text-embedding-ada-002` |\n",
        "\n",
        "---\n",
        "\n",
        "By default the maximum number of tokens used for completion is `256`. We can increase this upto the maximum calculated above of `3685`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb8fcda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb8fcda",
        "outputId": "7ce04009-aa5f-4e42-d83f-469a30ce00ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Import the `openai` library with pip, set the API key, and use the `Completion.create()` method to generate a response to a prompt: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)```\n",
            "\n",
            "2. Install the LangChain library with `pip install langchain openai`, import the library, and initialize the model with the API key: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n"
          ]
        }
      ],
      "source": [
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    temperature=0.0,\n",
        "    max_tokens=3685\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abee5eed",
      "metadata": {
        "id": "abee5eed"
      },
      "source": [
        "The model doesn't need the full size of completion and doesn't try to fill the full space, but because we increased the value of `openai.max_tokens`, inference does take notably longer.\n",
        "\n",
        "If we exceed the maximum context window allowed, we'll see an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5243e2b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5243e2b6",
        "outputId": "f038cb55-8b35-4806-c6f0-c7e93b7c5159",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This model's maximum context length is 4097 tokens, however you requested 4098 tokens (412 in your prompt; 3686 for the completion). Please reduce your prompt; or completion length.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    res = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.0,\n",
        "        max_tokens=3686\n",
        "    )\n",
        "except openai.InvalidRequestError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0cf275d",
      "metadata": {
        "id": "b0cf275d"
      },
      "source": [
        "So it can be a good idea to integrate this type of check into our code if we expect to exceed the maximum context window at any point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ebe2e9d-685d-4ea3-ab7c-9589caaabff5",
      "metadata": {
        "id": "7ebe2e9d-685d-4ea3-ab7c-9589caaabff5"
      },
      "source": [
        "# Prompting using Gpt3.5 turbo\n",
        "In this lesson, you'll practice prompting principles and their related tactics by using the GPT3.5 turbo\n",
        "\n",
        "## Setup\n",
        "#### Load the API key and relevant Python libaries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bab499-9a50-4bd0-a622-1c914c6ccc29",
      "metadata": {
        "id": "00bab499-9a50-4bd0-a622-1c914c6ccc29"
      },
      "source": [
        "In this course, we've provided some code that loads the OpenAI API key for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c382975",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 132,
        "id": "6c382975",
        "outputId": "6b6bc006-56e8-4225-9aab-80804b0b7875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7babef03ff60> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-babbage-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-babbage-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-babbage-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-davinci-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-davinci-edit-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-instruct-beta\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-ada-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-instruct-beta\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-babbage-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-davinci-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-curie-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-davinci-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-code-search-code\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"davinci-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie-search-document\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-code-search-code\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-ada-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-ada-text-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"babbage-code-search-text\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-babbage-code-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-search-query\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-code-search-text\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-curie-query-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-edit-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-babbage-text-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-ada-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"ada-similarity\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"code-search-ada-code-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-similarity-ada-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-16k\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-search-curie-doc-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-davinci-003\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"text-curie-001\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"curie\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-4-0314\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-0613\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"engine\",\n",
              "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
              "      \"ready\": true,\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"created\": null\n",
              "    }\n",
              "  ]\n",
              "}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3acefa8-f8f1-4ef8-932e-9bcefa142666",
      "metadata": {
        "id": "a3acefa8-f8f1-4ef8-932e-9bcefa142666"
      },
      "source": [
        "#### helper function\n",
        "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
        "\n",
        "This helper function will make it easier to use prompts and look at the generated outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7dff174",
      "metadata": {
        "height": 166,
        "id": "a7dff174"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87121316",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 353,
        "id": "87121316",
        "outputId": "094efde3-af96-49a2-9775-b7f713a0a0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To guide a model towards the desired output and minimize irrelevant or incorrect responses, it is important to provide clear and specific instructions, even if it means writing longer prompts that offer more clarity and context.\n"
          ]
        }
      ],
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2798f3d-7618-4ac5-a6b2-3c69c537903d",
      "metadata": {
        "id": "f2798f3d-7618-4ac5-a6b2-3c69c537903d"
      },
      "source": [
        "#### Tactic 2: Ask for a structured output\n",
        "- JSON, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b50bbbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 166,
        "id": "6b50bbbd",
        "outputId": "4883ef33-092d-406f-8991-93178c24969b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"books\": [\n",
            "    {\n",
            "      \"book_id\": 1,\n",
            "      \"title\": \"The Enigma of Elysium\",\n",
            "      \"author\": \"Aria Nightshade\",\n",
            "      \"genre\": \"Fantasy\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 2,\n",
            "      \"title\": \"Whispers in the Shadows\",\n",
            "      \"author\": \"Evelyn Blackwood\",\n",
            "      \"genre\": \"Mystery\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 3,\n",
            "      \"title\": \"Beyond the Veil\",\n",
            "      \"author\": \"Lucian Rivers\",\n",
            "      \"genre\": \"Horror\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along \\\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a71c4f-b1f1-4d67-ad5a-e49fc1e3147d",
      "metadata": {
        "id": "22a71c4f-b1f1-4d67-ad5a-e49fc1e3147d"
      },
      "source": [
        "#### Tactic 3: Ask the model to check whether conditions are satisfied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ae612e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "f0ae612e",
        "outputId": "a7790baa-03fc-404d-adf6-e34876682fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for Text 1:\n",
            "Step 1 - Get some water boiling.\n",
            "Step 2 - Grab a cup and put a tea bag in it.\n",
            "Step 3 - Pour the hot water over the tea bag.\n",
            "Step 4 - Let the tea steep for a few minutes.\n",
            "Step 5 - Take out the tea bag.\n",
            "Step 6 - Add sugar or milk to taste.\n",
            "Step 7 - Enjoy your cup of tea.\n"
          ]
        }
      ],
      "source": [
        "text_1 = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some \\\n",
        "water boiling. While that's happening, \\\n",
        "grab a cup and put a tea bag in it. Once the water is \\\n",
        "hot enough, just pour it over the tea bag. \\\n",
        "Let it sit for a bit so the tea can steep. After a \\\n",
        "few minutes, take out the tea bag. If you \\\n",
        "like, you can add some sugar or milk to taste. \\\n",
        "And that's it! You've got yourself a delicious \\\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 1:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b6cc59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "76b6cc59",
        "outputId": "292753a4-d994-429e-8a33-d688c95c3487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for Text 2:\n",
            "No steps provided.\n"
          ]
        }
      ],
      "source": [
        "text_2 = f\"\"\"\n",
        "The sun is shining brightly today, and the birds are \\\n",
        "singing. It's a beautiful day to go for a \\\n",
        "walk in the park. The flowers are blooming, and the \\\n",
        "trees are swaying gently in the breeze. People \\\n",
        "are out and about, enjoying the lovely weather. \\\n",
        "Some are having picnics, while others are playing \\\n",
        "games or simply relaxing on the grass. It's a \\\n",
        "perfect day to spend time outdoors and appreciate the \\\n",
        "beauty of nature.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 2:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5866b8-d8c7-4e19-93db-401315f64954",
      "metadata": {
        "id": "3c5866b8-d8c7-4e19-93db-401315f64954"
      },
      "source": [
        "#### Tactic 4: \"Few-shot\" prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ce1540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 268,
        "id": "82ce1540",
        "outputId": "00ae73d8-ff00-467e-8be2-bb455a22e6d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<grandparent>: Resilience is like a mighty oak tree that withstands the strongest storms, bending but never breaking. It is the ability to bounce back from adversity, to find strength in the face of challenges, and to persevere even when the odds seem insurmountable. Just as a diamond is formed under immense pressure, resilience is forged through the trials and tribulations of life.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "\n",
        "<grandparent>: The river that carves the deepest \\\n",
        "valley flows from a modest spring; the \\\n",
        "grandest symphony originates from a single note; \\\n",
        "the most intricate tapestry begins with a solitary thread.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece7a8ee-1a2d-415d-8c10-500ecff24b10",
      "metadata": {
        "id": "ece7a8ee-1a2d-415d-8c10-500ecff24b10"
      },
      "source": [
        "#### Tactic 5: Specify the steps required to complete a task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7d6860",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "5e7d6860",
        "outputId": "f3a333b1-0fc5-4f53-abc1-d8f3ab78ffeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for prompt 1:\n",
            "1 - Jack and Jill, siblings, go on a quest to fetch water from a well on a hill, but they both fall down the hill and return home slightly injured but still adventurous.\n",
            "\n",
            "2 - Jack et Jill, frère et sœur, partent à la recherche d'eau d'un puits situé au sommet d'une colline, mais ils tombent tous les deux et rentrent chez eux légèrement blessés mais toujours aventureux.\n",
            "\n",
            "3 - Jack, Jill.\n",
            "\n",
            "4 - {\n",
            "  \"french_summary\": \"Jack et Jill, frère et sœur, partent à la recherche d'eau d'un puits situé au sommet d'une colline, mais ils tombent tous les deux et rentrent chez eux légèrement blessés mais toujours aventureux.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\\n",
        "a quest to fetch water from a hilltop \\\n",
        "well. As they climbed, singing joyfully, misfortune \\\n",
        "struck—Jack tripped on a stone and tumbled \\\n",
        "down the hill, with Jill following suit. \\\n",
        "Though slightly battered, the pair returned home to \\\n",
        "comforting embraces. Despite the mishap, \\\n",
        "their adventurous spirits remained undimmed, and they \\\n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt_1)\n",
        "print(\"Completion for prompt 1:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bb1dcf-95f5-4ee1-8c25-8b2abd5f0f0d",
      "metadata": {
        "id": "d0bb1dcf-95f5-4ee1-8c25-8b2abd5f0f0d"
      },
      "source": [
        "#### Ask for output in a specified format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e4222cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 370,
        "id": "3e4222cc",
        "outputId": "de5c1e60-c962-429c-a4bb-4608b5f83d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completion for prompt 2:\n",
            "Summary: Jack and Jill go on a quest to fetch water from a hilltop well, but they both fall down the hill and return home slightly battered but still adventurous.\n",
            "Translation: Jack et Jill partent à la recherche d'eau d'un puits au sommet d'une colline, mais ils tombent tous les deux et rentrent chez eux légèrement blessés mais toujours aventureux.\n",
            "Names: Jack, Jill\n",
            "Output JSON: {\"french_summary\": \"Jack et Jill partent à la recherche d'eau d'un puits au sommet d'une colline, mais ils tombent tous les deux et rentrent chez eux légèrement blessés mais toujours aventureux.\", \"num_names\": 2}\n"
          ]
        }
      ],
      "source": [
        "prompt_2 = f\"\"\"\n",
        "Your task is to perform the following actions:\n",
        "1 - Summarize the following text delimited by\n",
        "  <> with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the\n",
        "  following keys: french_summary, num_names.\n",
        "\n",
        "Use the following format:\n",
        "Text: <text to summarize>\n",
        "Summary: <summary>\n",
        "Translation: <summary translation>\n",
        "Names: <list of names in Italian summary>\n",
        "Output JSON: <json with summary and num_names>\n",
        "\n",
        "Text: <{text}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt_2)\n",
        "print(\"\\nCompletion for prompt 2:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec80fdb-92db-48f6-8f1d-b03c26385bad",
      "metadata": {
        "id": "fec80fdb-92db-48f6-8f1d-b03c26385bad"
      },
      "source": [
        "#### Tactic 6: Instruct the model to work out its own solution before rushing to a conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5cc985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 438,
        "id": "ff5cc985",
        "outputId": "eaf82ca5-e06f-4040-d384-a7c1cf33bd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The student's solution is correct. They correctly identified the costs for land, solar panels, and maintenance, and calculated the total cost as a function of the number of square feet.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Determine if the student's solution is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f322ebd9-0f8a-43aa-97fe-5eac70cdcc6a",
      "metadata": {
        "id": "f322ebd9-0f8a-43aa-97fe-5eac70cdcc6a"
      },
      "source": [
        "#### Note that the student's solution is actually not correct.\n",
        "#### We can fix this by instructing the model to work out its own solution first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703f7003",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 1016,
        "id": "703f7003",
        "outputId": "ecd4772a-1a69-4bc7-c2a3-a058bd8b114f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\n",
            "\n",
            "Let x be the size of the installation in square feet.\n",
            "\n",
            "1. Land cost: $100 / square foot\n",
            "The cost of land is calculated by multiplying the size of the installation by the cost per square foot:\n",
            "Land cost = 100 * x\n",
            "\n",
            "2. Solar panel cost: $250 / square foot\n",
            "The cost of solar panels is calculated by multiplying the size of the installation by the cost per square foot:\n",
            "Solar panel cost = 250 * x\n",
            "\n",
            "3. Maintenance cost: $100,000 + $10 / square foot\n",
            "The maintenance cost is a flat fee of $100,000 per year, plus an additional $10 per square foot:\n",
            "Maintenance cost = 100,000 + 10 * x\n",
            "\n",
            "Total cost for the first year of operations:\n",
            "Total cost = Land cost + Solar panel cost + Maintenance cost\n",
            "Total cost = 100 * x + 250 * x + 100,000 + 10 * x\n",
            "Total cost = 360 * x + 100,000\n",
            "\n",
            "Is the student's solution the same as the actual solution just calculated:\n",
            "Yes\n",
            "\n",
            "Student grade:\n",
            "Correct\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to determine if the student's solution \\\n",
        "is correct or not.\n",
        "To solve the problem do the following:\n",
        "- First, work out your own solution to the problem.\n",
        "- Then compare your solution to the student's solution \\\n",
        "and evaluate if the student's solution is correct or not.\n",
        "Don't decide if the student's solution is correct until\n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "```\n",
        "question here\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the student's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Student grade:\n",
        "```\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I'm building a solar power installation and I need help \\\n",
        "working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations \\\n",
        "as a function of the number of square feet.\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "```\n",
        "Actual solution:\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a207eab-a1b1-47a5-b913-fe38086123d0",
      "metadata": {
        "id": "8a207eab-a1b1-47a5-b913-fe38086123d0"
      },
      "source": [
        "## Model Limitations: Hallucinations\n",
        "- Boie is a real company, the product name is not real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c80919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 115,
        "id": "81c80919",
        "outputId": "0564c8f7-0398-4bec-f2b0-2a14b9370aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The AeroGlide UltraSlim Smart Toothbrush by Boie is a technologically advanced toothbrush designed to provide a superior brushing experience. Boie is a company known for its innovative oral care products, and the AeroGlide UltraSlim Smart Toothbrush is no exception.\n",
            "\n",
            "One of the standout features of this toothbrush is its ultra-slim design. The brush head is only 2mm thick, making it much thinner than traditional toothbrushes. This slim profile allows for better access to hard-to-reach areas of the mouth, ensuring a thorough and effective clean.\n",
            "\n",
            "The AeroGlide UltraSlim Smart Toothbrush also incorporates smart technology. It connects to a mobile app via Bluetooth, allowing users to track their brushing habits and receive personalized recommendations for improving their oral hygiene routine. The app provides real-time feedback on brushing technique, ensuring that users are brushing for the recommended two minutes and covering all areas of their mouth.\n",
            "\n",
            "The toothbrush itself is made from a durable and hygienic material called thermoplastic elastomer. This material is non-porous, meaning it doesn't harbor bacteria or mold, making it more hygienic than traditional toothbrush bristles. The bristles are also ultra-soft, providing a gentle yet effective cleaning experience.\n",
            "\n",
            "In addition to its technological features, the AeroGlide UltraSlim Smart Toothbrush is also environmentally friendly. The brush head is replaceable, reducing waste compared to traditional toothbrushes. The toothbrush is also designed to last for up to six months, ensuring durability and longevity.\n",
            "\n",
            "Overall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology, slim design, and eco-friendly features. It aims to provide users with a superior brushing experience while promoting better oral health.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a354ce17",
      "metadata": {
        "id": "a354ce17"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "69df1e7d",
      "metadata": {
        "id": "69df1e7d"
      },
      "source": [
        "## Exercise Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b966d6",
      "metadata": {
        "id": "e8b966d6"
      },
      "source": [
        "### 1. `Inferring` you will infer sentiment and topics from product reviews and news articles.\n",
        "\n",
        "Review text.\n",
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "####  Task1\n",
        "1) Write prompt for Sentiment (positive/negative)\n",
        "2) Write prompt Identify types of emotions\n",
        "3) Write prompt Identify anger\n",
        "4) Write prompt Extract product and company name from customer reviews\n",
        "\n",
        "\n",
        "\n",
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "####  Task2\n",
        "\n",
        "1) Infer 5 topics that are being discussed in the given story"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iWbDhPn2bpLG",
      "metadata": {
        "id": "iWbDhPn2bpLG"
      },
      "source": [
        "## Task 1 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VUFiGPkpcDyl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUFiGPkpcDyl",
        "outputId": "251a6997-9f42-4838-829b-067fea519e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completion for prompt task 1 part 1:\n",
            "Sentiments are: Positive\n",
            "\n",
            "Completion for prompt task 1 part 2:\n",
            "No of Emotions: 3\n",
            "Emotions Types: Excitement, Satisfaction, Gratitude\n",
            "\n",
            "Completion for prompt task 1 part 3:\n",
            "Anger in review: No\n",
            "\n",
            "Completion for prompt task 1 part 4:\n",
            "Product: lamp\n",
            "Company: Lumina\n"
          ]
        }
      ],
      "source": [
        "lamp_review = \"\"\"Needed a nice lamp for my bedroom, and this one had\n",
        "additional storage and not too high of a price point.\n",
        "Got it fast. The string to our lamp broke during the\n",
        "transit and the company happily sent over a new one.\n",
        "Came within a few days as well. It was easy to put\n",
        "together. I had a missing part, so I contacted their\n",
        "support and they very quickly got me the missing piece!\n",
        "Lumina seems to me to be a great company that cares\n",
        "about their customers and products!!\"\"\"\n",
        "\n",
        "# 1) Write prompt for Sentiment (positive/negative)\n",
        "\n",
        "prompt1_task_1 = f\"\"\"\n",
        "Your task is to perform the following action:\n",
        "-> Perform the sentiment analysis of the following lamp review delimited by\n",
        "   <> and identify the whether the sentiments are positive or negative.\n",
        "\n",
        "Use the following format:\n",
        "Sentiments are: <Sentiments>\n",
        "\n",
        "lamp review: <{lamp_review}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt1_task_1)\n",
        "print(\"\\nCompletion for prompt task 1 part 1:\")\n",
        "print(response)\n",
        "\n",
        "# 2) Write prompt Identify types of emotions\n",
        "\n",
        "prompt2_task_1 = f\"\"\"\n",
        "Your will be provided by a customer review on a company service:\n",
        "-> Go through the following review delimited by \\\n",
        "   <> and pay special attentions to customer emotions to identify \\\n",
        "   the different type of emotions presented in their review.\n",
        "\n",
        "Use the following format:\n",
        "No of Emotions: <Number of different emotions in the review>\n",
        "Emotions Types: <Types of Emotions in the review>\n",
        "\n",
        "\n",
        "lamp review: <{lamp_review}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt2_task_1)\n",
        "print(\"\\nCompletion for prompt task 1 part 2:\")\n",
        "print(response)\n",
        "\n",
        "# 3) Write prompt Identify anger\n",
        "\n",
        "prompt3_task_1 = f\"\"\"\n",
        "Your task is to perform the following action:\n",
        "-> Perform the sentiment analysis of the following lamp review delimited by\n",
        "   <> and identify the whether there is anger in review or not.\n",
        "\n",
        "Use the following format:\n",
        "Anger in review: <Anger in review>\n",
        "\n",
        "lamp review: <{lamp_review}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt3_task_1)\n",
        "print(\"\\nCompletion for prompt task 1 part 3:\")\n",
        "print(response)\n",
        "\n",
        "# 4) Write prompt Extract product and company name from customer reviews\n",
        "\n",
        "prompt4_task_1 = f\"\"\"\n",
        "Your task is to extract the product and company from customer reviews:\n",
        "-> Perform the actionon the following review delimited by <>\n",
        "\n",
        "Use the following format:\n",
        "Product: <Product name>\n",
        "Company: <Company name>\n",
        "\n",
        "\n",
        "lamp review: <{lamp_review}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt4_task_1)\n",
        "print(\"\\nCompletion for prompt task 1 part 4:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QZFlbMYfr5Rs",
      "metadata": {
        "id": "QZFlbMYfr5Rs"
      },
      "source": [
        "## Task 2 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b60072",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95b60072",
        "outputId": "3e5804e5-570f-49af-c88a-05d8ca0fc7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completion for prompt task 2 part:\n",
            "Topics that are being discussed in the given survey:\n",
            "1. Satisfaction levels of public sector employees\n",
            "2. Departmental satisfaction ratings\n",
            "3. NASA as the most popular department with high satisfaction rating\n",
            "4. Employee comments on NASA's work environment and opportunities\n",
            "5. Social Security Administration's low satisfaction rating and government's commitment to address concerns\n"
          ]
        }
      ],
      "source": [
        "survey = \"\"\"\n",
        "In a recent survey conducted by the government, public sector employees were asked \\\n",
        "to rate their level of satisfaction with the department they work at. \\\n",
        "The results revealed that NASA was the most popular department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings, stating, \"I'm not surprised that NASA \\\n",
        "came out on top. It's a great place to work with amazing people and incredible opportunities. \\\n",
        "I'm proud to be a part of such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team, with Director Tom Johnson stating, \\\n",
        "\"We are thrilled to hear that our employees are satisfied with their work at NASA. We have a \\\n",
        "talented and dedicated team who work tirelessly to achieve our goals, and it's fantastic to see\\\n",
        "that their hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the Social Security Administration had the lowest \\\n",
        "satisfaction rating, with only 45% of employees indicating they were satisfied with \\\n",
        "their job. The government has pledged to address the concerns raised by employees in \\\n",
        "the survey and work towards improving job satisfaction across all departments.\n",
        "\"\"\"\n",
        "\n",
        "# Infer 5 topics that are being discussed in the given story\n",
        "\n",
        "\n",
        "prompt_task_2 = f\"\"\" Yow will be provided by a Survey on work satisfaction level of public sector employees.\n",
        "Your job is to perform the following tasks:\n",
        "1 - Indentify Different Topics that are mentioned in the {survey}.\n",
        "2 - Provide the list of five topics that are being discussed in the given survey.\n",
        "\n",
        "Follow the given structure:\n",
        "Topics that are being discussed in the given survey: <5 topics>\n",
        "Separate each topic by new line\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt_task_2)\n",
        "print(\"\\nCompletion for prompt task 2:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e74ad6e",
      "metadata": {
        "id": "5e74ad6e"
      },
      "source": [
        "## 2 `Expanding` you will generate customer service emails that are tailored to each customer's review.\n",
        "\n",
        "review = \"\"\"So, they still had the 17 piece system on seasonal \\\n",
        "sale for around 49 dollar in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between 70-89 dollar for the same \\\n",
        "system. And the 11 piece system went up around 10 dollar or \\\n",
        "so in price also from the earlier sale price of 29 dollar. \\\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy). Special tip when making \\\n",
        "smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie. \\\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone done in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\"\"\"\n",
        "\n",
        "####  Task1\n",
        "1) Write prompt for Customize the automated reply to a customer email and remind the model to use details from the customer's email\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XVdDH5EP9Rfy",
      "metadata": {
        "id": "XVdDH5EP9Rfy"
      },
      "source": [
        "## Customer Service Email Task 3 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ec906b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ec906b",
        "outputId": "4ed5dd9d-1402-4c2e-bd94-cc17435b0e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Customer Service Email tailored to the Customer's Review:\n",
            "\n",
            "Dear valued customer,\n",
            "\n",
            "Thank you for taking the time to share your feedback with us. We appreciate your support and loyalty to our brand.\n",
            "\n",
            "We apologize for any inconvenience you may have experienced regarding the pricing of our 17 piece system. Our prices are subject to change based on various factors, including seasonal sales and market conditions. However, we understand your concern and will take it into consideration for future pricing decisions.\n",
            "\n",
            "Regarding the base of the system, we appreciate your feedback on the locking mechanism. We continuously strive to improve the quality of our products, and your input will be shared with our product development team for further evaluation.\n",
            "\n",
            "Thank you for sharing your special tip for making smoothies. We value your expertise and will definitely consider it for our future recipe recommendations.\n",
            "\n",
            "We are sorry to hear about the issue you faced with the motor of your previous blender. As the warranty had expired, we understand your frustration. If you encounter any issues with your new blender, please do not hesitate to reach out to our customer service team for assistance.\n",
            "\n",
            "Once again, thank you for your feedback and for choosing our products. We truly value your support.\n",
            "\n",
            "Regards,\n",
            "Customer Service Team\n"
          ]
        }
      ],
      "source": [
        "review = \"\"\"So, they still had the 17 piece system on seasonal \\\n",
        "sale for around 49 dollar in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between 70-89 dollar for the same \\\n",
        "system. And the 11 piece system went up around 10 dollar or \\\n",
        "so in price also from the earlier sale price of 29 dollar.\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy).\n",
        "Special tip when making smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie.\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone done in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\"\"\"\n",
        "\n",
        "# Write prompt for Customize the automated reply to a customer email and remind the model to use details from the customer's email\n",
        "\n",
        "prompt_task_3 = f\"\"\"\n",
        "                You will be provided by customers reviews that are delimited by <>, and you have to responde with custom \\\n",
        "customer service emails tailored to their reviews.\n",
        "\n",
        "Keep these things into consideration:\n",
        "-> If name of customer is given only then mention their name in the start.\n",
        "-> Email should be to the point.\n",
        "-> Maintain proper sentence spaces.\n",
        "\n",
        "review: <{review}>\n",
        "\n",
        "Follow this Format:\n",
        "1 - Greet the reviewer with Positive sentiments.\n",
        "2 - Analyze the review, whether its positive or negative.\n",
        "3 - If positive then appreciate, and in case of negative appologies and offer assistance.\n",
        "4 - Pay attention to details of review that are relavent to customer service, then responde shortly, and accordingly.\n",
        "5 - Finish email with Regards with Customer Service Team.\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt_task_3)\n",
        "print(\"\\nCustomer Service Email tailored to the Customer's Review:\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4592125f",
      "metadata": {
        "id": "4592125f"
      },
      "source": [
        "## 3 Summarizing you will summarize text with a focus on specific topics.\n",
        "\n",
        "prod_review = \"\"\"\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\n",
        "\"\"\"\n",
        "\n",
        "### Task\n",
        "\n",
        "1) Write a prompt to summarize it with focus on delivery and shipping\n",
        "2) Write a prompt to summarize it with focus on price and value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ny3pjyJ0aaof",
      "metadata": {
        "id": "ny3pjyJ0aaof"
      },
      "source": [
        "\n",
        "## Summarization Task 4 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ff315b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ff315b",
        "outputId": "07f7b00d-d15d-4b19-91c3-96002cb032c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary with focus on delivery and shipping:\n",
            "\n",
            "The panda plush toy arrived a day earlier than expected, allowing the customer to play with it before giving it to their daughter for her birthday.\n",
            "\n",
            "\n",
            "Summary with focus on price and value:\n",
            "\n",
            "The panda plush toy is loved by the daughter and has a friendly look, but the customer feels it is slightly overpriced and suggests there may be larger options available for the same price.\n"
          ]
        }
      ],
      "source": [
        "prod_review = \"\"\"Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look.\n",
        "It's a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price.\n",
        "It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her. \"\"\"\n",
        "\n",
        "# 1) Write a prompt to summarize it with focus on delivery and shipping\n",
        "prompt1_task_4 = f\"\"\"Your task is to summerize the text given in {prod_review} in one sentence:\n",
        "\n",
        "The summery should be focused on:\n",
        "-> Product Delivery and product Shipping\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt1_task_4)\n",
        "print(\"\\nSummary with focus on delivery and shipping:\\n\")\n",
        "print(response)\n",
        "\n",
        "# 2) Write a prompt to summarize it with focus on price and value\n",
        "prompt2_task_4 = f\"\"\"Your task is to summerize the text given in {prod_review} in one sentence:\n",
        "\n",
        "The summery should be focused on:\n",
        "1 - Product price\n",
        "2 - Product value\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt2_task_4)\n",
        "print(\"\\n\\nSummary with focus on price and value:\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16b1fd0b",
      "metadata": {
        "id": "16b1fd0b"
      },
      "source": [
        "# Transforming use Large Language Models for text transformation tasks such as language translation, spelling and grammar checking, tone adjustment, and format conversion.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1\n",
        "'Dude, This is Joe, check out this spec on this standing lamp.'\n",
        "1) Write prompt toransform this text into formal tone\n",
        "\n",
        "2) Write a prompt to transform and check for the spelling and Homonyms\n",
        "\n",
        "    Text =\n",
        "  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
        "  \"Yolanda has her notebook.\", # ok\n",
        "  \"Its going to be a long day. Does the car need it’s oil changed?\",  # Homonyms\n",
        "  \"Their goes my freedom. There going to bring they’re suitcases.\",  # Homonyms\n",
        "  \"Your going to need you’re notebook.\",  # Homonyms\n",
        "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
        "  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n",
        "  \n",
        "3) Write a prompt to translate all given text to english\n",
        "\n",
        "text =   \"La performance du système est plus lente que d'habitude.\",  \n",
        "  \"Mi monitor tiene píxeles que no se iluminan.\",              \n",
        "  \"Il mio mouse non funziona\",                               \n",
        "  \"Mój klawisz Ctrl jest zepsuty\",                            \n",
        "  \"我的屏幕在闪烁\"                                               \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xk2fpKgskO33",
      "metadata": {
        "id": "Xk2fpKgskO33"
      },
      "source": [
        "# **Transforming use Large Language Models for text transformation**\n",
        "\n",
        "## Formal Tone Task 1 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9951f7aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9951f7aa",
        "outputId": "fcdb4918-8beb-4c66-d200-f099e8432b3d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Coversion of informal text to formal text:\n",
            "\n",
            "Hello, this is Joe. I would like to draw your attention to this specification for the standing lamp.\n"
          ]
        }
      ],
      "source": [
        "text_1 = \"Dude, This is Joe, check out this spec on this standing lamp.\"\n",
        "\n",
        "#  1) Write prompt to transform this text into formal tone\n",
        "prompt1_task_5 = f\"\"\"Your task is to convert the informal text into one with formal tone:\n",
        "\n",
        "Covert the text delimited by <> into the equavalent formal text.\n",
        "\n",
        "text: <{text_1}>\n",
        "\n",
        "\"\"\"\n",
        "# using gpt 3.5 turbo\n",
        "response = get_completion(prompt1_task_5)\n",
        "print(\"\\nCoversion of informal text to formal text:\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xBtUadlKXt02",
      "metadata": {
        "id": "xBtUadlKXt02"
      },
      "source": [
        "## Same Formal Tone Task using Davinci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kQIMQvjznAes",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQIMQvjznAes",
        "outputId": "6e6af2eb-60c6-4b9a-a384-164cf36b304b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good day, my name is Joe. I would like to draw your attention to the specifications of this standing lamp.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name='text-davinci-003')\n",
        "\n",
        "temp = \"\"\"Your task is to convert the informal text into one with formal tone:\n",
        "\n",
        "\n",
        "text: {informal_text}\n",
        "\n",
        "formal text:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt_for_davinci = PromptTemplate(\n",
        "    template = temp,\n",
        "    input_variables = [\"informal_text\"]\n",
        ")\n",
        "\n",
        "\n",
        "davinci = LLMChain(\n",
        "    prompt=prompt_for_davinci,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "text_to_conv = \"Dude, This is Joe, check out this spec on this standing lamp.\"\n",
        "\n",
        "print(davinci.run(text_to_conv))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H7vUJIy3aLyG",
      "metadata": {
        "id": "H7vUJIy3aLyG"
      },
      "source": [
        "## Transform and check for the spelling and Homonyms Task 2 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N6jFtTnEaRyW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6jFtTnEaRyW",
        "outputId": "9d7ae809-015e-4e12-e8b6-a871cdb79041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform and check for the spelling and Homonyms:\n",
            "\n",
            "1 - List of spelling mistakes in the text:\n",
            "- cherck (check)\n",
            "- chatGPT (ChatGPT)\n",
            "- speling (spelling)\n",
            "- abilitty (ability)\n",
            "\n",
            "2 - List of Homonyms in the text:\n",
            "- its/it's\n",
            "- there/their/they're\n",
            "- your/you're\n",
            "- affect/effect\n",
            "\n",
            "3 - Corrected spelling mistakes in the text:\n",
            "- \"This phrase is to check ChatGPT for spelling ability\"\n",
            "\n",
            "4 - Corrected grammatical mistakes in the text:\n",
            "- \"The girl with the black and white puppies has a ball.\"\n",
            "- \"Yolanda has her notebook.\"\n",
            "- \"It's going to be a long day. Does the car need its oil changed?\"\n",
            "- \"There goes my freedom. They're going to bring their suitcases.\"\n",
            "- \"You're going to need your notebook.\"\n",
            "- \"That medicine affects my ability to sleep. Have you heard of the butterfly effect?\"\n",
            "- \"This phrase is to check ChatGPT for spelling ability\"\n",
            "\n",
            "5 - Generated fine text line by line:\n",
            "- The girl with the black and white puppies has a ball.\n",
            "- Yolanda has her notebook.\n",
            "- It's going to be a long day. Does the car need its oil changed?\n",
            "- There goes my freedom. They're going to bring their suitcases.\n",
            "- You're going to need your notebook.\n",
            "- That medicine affects my ability to sleep. Have you heard of the butterfly effect?\n",
            "- This phrase is to check ChatGPT for spelling ability.\n"
          ]
        }
      ],
      "source": [
        "text_2 = \"\"\"The girl with the black and white puppies have a ball.\", # The girl has a ball\n",
        "\"Yolanda has her notebook.\", # ok\n",
        "\"Its going to be a long day. Does the car need it’s oil changed?\", # Homonyms\n",
        "\"Their goes my freedom. There going to bring they’re suitcases.\", # Homonyms\n",
        "\"Your going to \\ need you’re notebook.\", # Homonyms\n",
        "\"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
        "\"This phrase is to cherck chatGPT for speling abilitty\" # spelling\"\"\"\n",
        "\n",
        "# 2) Write a prompt to transform and check for the spelling and Homonyms\n",
        "\n",
        "prompt2_task_5 = f\"\"\"You will be provided by a text delimited by <> and you have to perform certain task on it:\n",
        "\n",
        "These are the following task you should perform:\n",
        "1 - Make list of spelling mistakes in the text.\n",
        "2 - Make list of Homonyms in the text.\n",
        "3 - Correct the spelling mistakes.\n",
        "4 - Correct the grammatical mistakes.\n",
        "5 - Generate a fine text line by line.\n",
        "\n",
        "text: <{text_2}>\n",
        "\n",
        "Here are some examples of Homonyms:\n",
        "\n",
        "(Bat: Bats are mammals that can fly.\n",
        " Bat: A bat is also a piece of sports equipment used in games like baseball.)\n",
        "\n",
        "(Week: A period of seven days.\n",
        " Weak: Lacking in strength or power.)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# using gpt 3.5 turbo\n",
        "response = get_completion(prompt2_task_5)\n",
        "print(\"Transform and check for the spelling and Homonyms:\\n\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qhVOFm41luVh",
      "metadata": {
        "id": "qhVOFm41luVh"
      },
      "source": [
        "## Task 3 Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "teFw8aFilzCq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teFw8aFilzCq",
        "outputId": "33101e9d-7021-4392-851e-826421f09613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Translation:\n",
            "\"The system performance is slower than usual.\"\n",
            "\"My monitor has pixels that do not light up.\"\n",
            "\"My mouse is not working.\"\n",
            "\"My Ctrl key is broken.\"\n",
            "\"My screen is flickering.\"\n"
          ]
        }
      ],
      "source": [
        "# 3) Write a prompt to translate all given text to english\n",
        "\n",
        "text_3 = \"\"\"\n",
        "\"La performance du système est plus lente que d'habitude.\",\n",
        "\"Mi monitor tiene píxeles que no se iluminan.\",\n",
        "\"Il mio mouse non funziona\",\n",
        "\"Mój klawisz Ctrl jest zepsuty\",\n",
        "\"我的屏幕在闪烁\"\n",
        "\"\"\"\n",
        "\n",
        "prompt3_task_5 = f\"\"\"\n",
        "You will be provided with texts from various languages delimited by <> and your task is to translate all of them into English:\n",
        "\n",
        "text: <{text_3}>\n",
        "\n",
        "Follow this Format:\n",
        "English Translation: Translation for each line\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# using gpt 3.5 turbo\n",
        "response = get_completion(prompt3_task_5)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DJgK_EEyrLgk",
      "metadata": {
        "id": "DJgK_EEyrLgk"
      },
      "source": [
        "## Same Translation Task with Curie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HDl5EofVrRgP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDl5EofVrRgP",
        "outputId": "8f045c1b-79d6-4339-ecc1-569fca391550",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Translation:\n",
            "\n",
            "\"The performance of the system is slower than usual.\"\n",
            "\"My monitor has pixels that don't light up.\"\n",
            "\"My mouse isn't working.\"\n",
            "\"My Ctrl key is broken.\"\n",
            "\"My monitor is flickering.\"\n"
          ]
        }
      ],
      "source": [
        "llm_davinci = OpenAI(model_name='text-curie-001')\n",
        "\n",
        "translation_prompt = \"\"\"\n",
        "Task is English Translation:\n",
        "\n",
        "text: {text}\n",
        "\n",
        "Translation:\n",
        "\"\"\"\n",
        "\n",
        "translation_prompt = PromptTemplate(\n",
        "    template=translation_prompt,\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "llm_davinci = LLMChain(\n",
        "    prompt=translation_prompt,\n",
        "    llm=llm_davinci\n",
        ")\n",
        "\n",
        "print(\"English Translation:\\n\" + llm_davinci.run(text_3))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}