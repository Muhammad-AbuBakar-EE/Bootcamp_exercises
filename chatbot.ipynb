{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Muhammad Abu Bakar**"
      ],
      "metadata": {
        "id": "ycc6-gzV1q2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain\n",
        "!pip install -qU openai\n",
        "!pip install -qU langchain-community\n",
        "!pip install -qU langchain-experimental"
      ],
      "metadata": {
        "id": "N42rqs0mwI7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37612352-01be-428b-e756-15286a6b0a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "hsSYq831sB7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rDqKZyBvcDq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "ANY57jOPEgZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "id": "E7L18ObqElDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "ZF5u_CuyDdCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFyBAL4DS7g",
        "outputId": "a2afa269-0fb0-4b8d-b909-ee5a380ddffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.llm_bash.base import LLMBashChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_experimental.llm_bash.prompt import BashOutputParser\n",
        "from langchain_experimental.llm_bash.base import LLMBashChain"
      ],
      "metadata": {
        "id": "Ub7WOrcjDVdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain openai tiktoken"
      ],
      "metadata": {
        "id": "c7PF7P_gi44q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "5rwvd2Nmi5iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(openai_api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "N--PMqqVjGiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "J0y11ulQji_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary Buffer Memory**"
      ],
      "metadata": {
        "id": "ouClijdZs1ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
      ],
      "metadata": {
        "id": "cxEMLWGsjnRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tranform Chain Functions**"
      ],
      "metadata": {
        "id": "uQK8C_-_tHyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_func_a(inputs: dict)-> dict:\n",
        "  question = inputs[\"input\"]\n",
        "  answer = inputs[\"text\"]\n",
        "  substring_list = answer.split(':')\n",
        "  memory.save_context({\"input\": question}, {\"output\": substring_list[1]})\n",
        "  return {\"history_a\": inputs}"
      ],
      "metadata": {
        "id": "f5ZjclHZjIrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_func_b(inputs: dict)-> dict:\n",
        "  question = inputs[\"input\"]\n",
        "  answer = inputs[\"text\"]\n",
        "  memory.save_context({\"input\": question}, {\"output\": answer})\n",
        "  return {\"history_b\": inputs}"
      ],
      "metadata": {
        "id": "ipnfkLQ9jJWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_func_c(inputs: dict)-> dict:\n",
        "  question = inputs[\"input\"]\n",
        "  feed_back = memory.load_memory_variables(inputs = [])['history']\n",
        "  question = f\"{feed_back} \\n{question}\"\n",
        "  return {\"history_c\": question}"
      ],
      "metadata": {
        "id": "q-0SqnMAjM0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Info**"
      ],
      "metadata": {
        "id": "zjJxItQ6tUVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"bash\",\n",
        "        \"description\": \"Good for writing bash scripts\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "gpTeZixDErx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transform Chains**"
      ],
      "metadata": {
        "id": "-iC8m3lltZIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_chain_one = TransformChain(input_variables=[\"text\"], output_variables=[\"history_a\"], transform=transform_func_a)"
      ],
      "metadata": {
        "id": "4_Aot2xzjcBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_chain_two = TransformChain(input_variables=[\"text\"], output_variables=[\"history_b\"], transform=transform_func_b)"
      ],
      "metadata": {
        "id": "GMiOI44tjczG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_chain_three = TransformChain(input_variables=[\"input\"], output_variables=[\"history_c\"], transform=transform_func_c)"
      ],
      "metadata": {
        "id": "EhvsTCkWjez3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM MATH CHAIN**"
      ],
      "metadata": {
        "id": "XPpZrrhVtgCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_math = LLMMathChain(llm=llm, input_key = \"input\", output_key = \"text\", verbose=False)"
      ],
      "metadata": {
        "id": "cz92hw6djsPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM BASH CHAIN**"
      ],
      "metadata": {
        "id": "zuykrXYUtn1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_bash = LLMBashChain(llm=llm, input_key = \"input\", output_key =\"text\",verbose=True)"
      ],
      "metadata": {
        "id": "vj3wBRlLjs_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Destination Chains**"
      ],
      "metadata": {
        "id": "ej3vT6ertrSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    if name == \"math\":\n",
        "      chain = SequentialChain(chains=[transform_chain_three, llm_math, transform_chain_one], input_variables=[\"input\"], output_variables=[\"text\"])\n",
        "      destination_chains[name] = chain\n",
        "    if name == \"bash\":\n",
        "      chain = SequentialChain(chains=[transform_chain_three, llm_bash, transform_chain_two], input_variables=[\"input\"], output_variables=[\"text\"])\n",
        "      destination_chains[name] = chain\n",
        "default_chain = ConversationChain(llm=llm, memory = memory, output_key = \"text\", verbose=False)"
      ],
      "metadata": {
        "id": "ryiNbiAsv738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMRouterChain**"
      ],
      "metadata": {
        "id": "LaIS53zwtzgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "YkgTbpP5wA0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MultiPromptChain**"
      ],
      "metadata": {
        "id": "yjp4YU7Zt9Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "j5MUMnUzwCV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function to Activate Chat Bot**"
      ],
      "metadata": {
        "id": "moGRclmsuFcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_call_function():\n",
        "  input_from_user = input(f\"To exit write exit:\\nHuman: \")\n",
        "  if  input_from_user.lower() == \"exit\":\n",
        "    print(\"Exiting recursion.\")\n",
        "    return\n",
        "  result = chain.run(input_from_user)\n",
        "  print(f\"\\nAI: {result}\\n\")\n",
        "  chat_call_function()"
      ],
      "metadata": {
        "id": "OI6TTomQmQxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Complete Chat**"
      ],
      "metadata": {
        "id": "N2JRxAhVwy0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_call_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvTrt1MhkA7w",
        "outputId": "5a8720a9-d7e9-44c6-9604-20323ade639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To exit write exit:\n",
            "Human: Write me bash script of \"for loop\".\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Write me a bash script of a 'for loop'\u001b[32;1m\u001b[1;3m to print out the numbers 1 to 10\n",
            "\n",
            "```bash\n",
            "for i in {1..10}; do\n",
            "  echo $i\n",
            "done\n",
            "```\u001b[0m\n",
            "Code: \u001b[33;1m\u001b[1;3m['for i in {1..10}; do', '  echo $i', 'done']\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3mCommand 'for i in {1..10}; do;  echo $i;done' returned non-zero exit status 2.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "AI: Command 'for i in {1..10}; do;  echo $i;done' returned non-zero exit status 2.\n",
            "\n",
            "To exit write exit:\n",
            "Human: The square of 177 is\n",
            "\n",
            "AI: Answer: 31329\n",
            "\n",
            "To exit write exit:\n",
            "Human: The cube root of 67\n",
            "\n",
            "AI: Answer: 4.0615481004456795\n",
            "\n",
            "To exit write exit:\n",
            "Human: nice, give me a summary of our chat.\n",
            "\n",
            "AI:  We discussed Islamabad and Karachi as two cities in Pakistan, and you asked me to write a bash script of a for loop, which returned a non-zero exit status 2. You then asked me to calculate the square root of 138, to which I responded with 11.74734012447073; the square of 156, to which I responded with 24336; and the cube root of 67, to which I responded with 4.0615481004456795.\n",
            "\n",
            "To exit write exit:\n",
            "Human: Thanks\n",
            "\n",
            "AI:  You're welcome! Is there anything else I can help you with?\n",
            "\n",
            "To exit write exit:\n",
            "Human: exit\n",
            "Exiting recursion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary of History**"
      ],
      "metadata": {
        "id": "2g2W4kX-uNDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feed_back = memory.load_memory_variables(inputs = [])['history']\n",
        "print(feed_back)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3DZyoKrlrmr",
        "outputId": "c4c45b5e-cd39-4064-e793-b97afb344347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: \n",
            "The human asked the AI to write a bash script for a for loop, which returned a non-zero exit status 2. The AI was then asked to calculate the square root of 138, and responded with 11.74734012447073. The human then asked the AI to write a bash script for a while loop, which again returned a non-zero exit status 2. The human then requested that the AI calculate the square of 156, to which the AI responded with 24336, and when asked to name any two cities in Pakistan, the AI responded with Islamabad and Karachi. The AI confirmed that they had discussed Islamabad and Karachi earlier, and also confirmed that the human had asked for a for loop and square root calculation. The AI then responded with the square of 177 and cube root of 67, both correctly.\n",
            "Human: Thanks\n",
            "AI:  You're welcome! Is there anything else I can help you with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **No of Used Token**"
      ],
      "metadata": {
        "id": "iMs6-uuhufKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
        "print(f'Summary Buffer memory conversation length: {len(tokenizer.encode(feed_back))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIcaMKLQIVyN",
        "outputId": "ac5dc682-da1c-4b11-9fb8-ef4fea169b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Buffer memory conversation length: 191\n"
          ]
        }
      ]
    }
  ]
}